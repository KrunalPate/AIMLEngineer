{
    "AWSTemplateFormatVersion": "2010-09-09",
    "Description": "AWS CloudFormation template to copy data to Datalake buckets (qs-1nlkhq1no)",
    "Resources": {
        "CopyData": {
            "Type": "AWS::CloudFormation::CustomResource",
            "Properties": {
                "ServiceToken": {
                    "Fn::GetAtt": [
                        "CopyDataFunction",
                        "Arn"
                    ]
                },
                "SubmissionsBucket": {
                    "Ref": "SubmissionsBucketName"
                },
                "DatasetBucket": {
                    "Ref": "DatasetS3BucketName"
                },
                "DatasetBucketPrefix": {
                    "Ref": "DatasetS3KeyPrefix"
                }
            }
        },
        "CopyDataFunction": {
            "Type": "AWS::Lambda::Function",
            "Properties": {
                "Code": {
                    "ZipFile": {
                        "Fn::Join": [
                            "\n",
                            [
                                "import os",
                                "from concurrent.futures import ThreadPoolExecutor",
                                "",
                                "import boto3",
                                "import cfnresponse",
                                "import functools",
                                "from botocore.exceptions import ClientError",
                                "",
                                "def copy_data(event, source_key, destination_key):",
                                "    submissions_bucket = boto3.resource('s3').Bucket(event['ResourceProperties']['SubmissionsBucket'])",
                                "    copy_source = {",
                                "        'Bucket': event['ResourceProperties']['DatasetBucket'],",
                                "        'Key': source_key",
                                "    }",
                                "    return functools.partial(submissions_bucket.copy, copy_source, destination_key)",
                                "",
                                "def recursive_copy_data(event, source_prefix, destination_prefix):",
                                "    data_bucket = boto3.resource('s3').Bucket(event['ResourceProperties']['DatasetBucket'])",
                                "    base_path = event['ResourceProperties']['DatasetBucketPrefix']",
                                "    source_path = os.path.join(base_path, source_prefix)",
                                "    for obj in data_bucket.objects.filter(Prefix=source_path):",
                                "        source_key = obj.key",
                                "        destination_key = os.path.join(destination_prefix, os.path.basename(obj.key))",
                                "        yield copy_data(event, source_key, destination_key)",
                                "",
                                "def generate_copy_jobs(event):",
                                "    base_path = event['ResourceProperties']['DatasetBucketPrefix']",
                                "    yield copy_data(",
                                "        event,",
                                "        source_key=os.path.join(base_path, 'demographics20170520.zip'),",
                                "        destination_key='demographics/2017/06/02/demographics20170520.zip'",
                                "    )",
                                "    yield copy_data(",
                                "        event,",
                                "        source_key=os.path.join(base_path, 'customers.csv'),",
                                "        destination_key='customers/2017/06/01/customers.csv'",
                                "    )",
                                "    yield copy_data(",
                                "        event,",
                                "        source_key=os.path.join(base_path, 'customers_metadata.json'),",
                                "        destination_key='metadata/customers_metadata.json'",
                                "    )",
                                "    yield from recursive_copy_data(event, source_prefix='orders/orders.csv', destination_prefix='orders/2017/06/01/')",
                                "    yield from recursive_copy_data(event, source_prefix='products/products.csv', destination_prefix='products/2017/06/01/')",
                                "",
                                "def handler(event, context):",
                                "    if event['RequestType'] != 'Create':",
                                "        return cfnresponse.send(event, context, cfnresponse.SUCCESS, {})",
                                "    try:",
                                "        with ThreadPoolExecutor(max_workers=7) as executor:",
                                "            futures = [executor.submit(job) for job in generate_copy_jobs(event)]",
                                "        for future in futures:",
                                "            exception = future.exception()",
                                "            if exception is not None:",
                                "                raise exception",
                                "        return cfnresponse.send(event, context, cfnresponse.SUCCESS, {})",
                                "    except ClientError as e:",
                                "        print(e)",
                                "        return cfnresponse.send(event, context, cfnresponse.FAILED, {})"
                            ]
                        ]
                    }
                },
                "Handler": "index.handler",
                "Role": {
                    "Ref": "CopyDataRoleARN"
                },
                "MemorySize": 512,
                "Runtime": "python3.6",
                "Timeout": 300
            }
        }
    },
    "Parameters": {
        "CopyDataRoleARN": {
            "Description": "CopyDataRole ARN",
            "Type": "String"
        },
        "DatasetS3BucketName": {
            "Description": "Dataset bucket name for the Quick Start dataset.",
            "Type": "String"
        },
        "DatasetS3KeyPrefix": {
            "Description": "S3 key prefix for the Quick Start dataset.",
            "Type": "String"
        },
        "SubmissionsBucketName": {
            "Description": "SubmissionsBucket bucket name",
            "Type": "String"
        }
    }
}